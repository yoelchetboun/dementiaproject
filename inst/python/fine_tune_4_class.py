# -*- coding: utf-8 -*-
"""fine-tune-4-class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DfIpm_r86aKJeSZF4F8we-4JO3VnyWxJ
"""

import numpy as np
import h5py as h5py
import matplotlib.pyplot as plt
import tensorflow as tf

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense
from keras import applications, optimizers
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix


# dimensions of our images.
img_width, img_height = 160, 224

train_data_dir = '/content/drive/MyDrive/CEPE/dementiaproject/3-class-bal/data_augment_train_more_cut_3_class'
validation_data_dir = '/content/drive/MyDrive/CEPE/dementiaproject/3-class-bal/data_augment_valid_more_cut_3_class'
test_data_dir = '/content/drive/MyDrive/CEPE/dementiaproject/3-class-bal/data_augment_test_more_cut_3_class'
test_data_dir = '/content/drive/MyDrive/CEPE/dementiaproject/3-class-bal/test_shiny'

nb_train_samples = 6997 
train_nb_cdr00 = 6604
train_nb_cdr05 = 1280
train_nb_cdr10 = 467
nb_validation_samples = 2254 
nb_test_samples = 72 #5225 
epochs = 100
batch_size = 40

from google.colab import drive
drive.mount('/content/drive')

train_datagen = ImageDataGenerator(
    rescale=1. / 255, 
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.15,
    zoom_range=0.2,
    channel_shift_range = 150,
    fill_mode='nearest')

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle = False,
    class_mode='categorical')

print(train_generator.class_indices)

"""Calcul du poids des classes"""

from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(train_generator.classes),
                                                 train_generator.classes)
class_weights = dict(enumerate(class_weights))
print(class_weights)

"""Architecture du modÃ¨le"""

np.random.seed(2929)

vgg_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))
print('Model loaded.')

# initialise top model
top_model = Sequential()
top_model.add(Flatten(input_shape=vgg_model.output_shape[1:]))
top_model.add(Dense(256, activation='relu'))
top_model.add(Dropout(0.5))
top_model.add(Dense(3, activation='softmax'))

model = Model(inputs=vgg_model.input, outputs=top_model(vgg_model.output))


model.trainable = True
# first train only classif part
for layer in model.layers[:19]:
    layer.trainable = False
    
model.summary()

METRICS = [tf.keras.metrics.Precision(name='precision'),
           tf.keras.metrics.Recall(name='recall'),
           tf.keras.metrics.AUC(name='auc'),
           tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None)]

sgd = optimizers.Adam(lr=2e-5)  # optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)

model.compile(loss="categorical_crossentropy",
              optimizer=sgd,
              metrics=METRICS
              )

"""Training partie extract feature"""

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,
                                                     restore_best_weights=True)
history = model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    callbacks = early_stopping_cb,
    class_weight = class_weights,
    verbose=1)

# save / load model feature extraction
model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/feature_extract_3_class_imbal/vgg_2e5_3_class_run4/')
model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/feature_extract_3_class_imbal/vgg_2e5_3_class_run4/vgg_2e5_3_class_run4.h5')

import pandas as pd

# convert the history.history dict to a pandas DataFrame:     
hist_df = pd.DataFrame(history.history) 

# save to json:  
hist_json_file = '/content/drive/MyDrive/CEPE/dementiaproject/models_colab/feature_extract_3_class_imbal/vgg_2e5_3_class_run4/history.json' 
with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)

# or save to csv: 
hist_csv_file = '/content/drive/MyDrive/CEPE/dementiaproject/models_colab/feature_extract_3_class_imbal/vgg_2e5_3_class_run4/history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

import json

#history = json.load('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/feature_extract_3_class_imbal/vgg_2e5/history.json' )

history_dict = history.history

# Plotting the training and validation loss
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs_0 = range(1, len(history_dict['categorical_accuracy']) + 1)
plt.plot(epochs_0, loss_values, 'bo', label='Training loss')
plt.plot(epochs_0, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss (feature-extract)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


# Plotting the training and validation accuracy
acc_values = history_dict['categorical_accuracy']
val_acc_values = history_dict['val_categorical_accuracy']
plt.plot(epochs_0, acc_values, 'bo', label='Training acc')
plt.plot(epochs_0, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy (feature-extract)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# save / load model feature extraction 
#model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/model_fine_tune_4_class')
#model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/model_fine_tune_4_class.h5')

from keras.models import load_model

model = load_model('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/fine_tuning_3_class_imbal/vgg_3block_3_class_run3/')

"""Training partie Fine Tuning :"""

#Fine tunning


# Total of 20 layers. The classification is considered as one layer
# Therefore, intermediate is 19 layers
# 0, 1[:4], 2[:7], 3[:11], 4[:15], 5[:19] (Group 0, 1, 2, 3, 4, 5)
# 0 -> All trainable
# 5 -> All non-trainable except classification layer
# Always keep layer 20 trainable because it is classification layer

model.trainable = True

for layer in model.layers[:7]:
       layer.trainable = False

METRICS = [tf.keras.metrics.Precision(name='precision'),
           tf.keras.metrics.Recall(name='recall'),
           tf.keras.metrics.AUC(name='auc'),
           tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None)]

sgd = optimizers.Adam(lr=1e-6)  # optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)

model.compile(loss="categorical_crossentropy",
              optimizer=sgd,
              metrics=METRICS
              )

model.summary()

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,
                                                     restore_best_weights=True)
history = model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    callbacks = early_stopping_cb,
    class_weight = class_weights,
    verbose=1)


# save / load model feature extraction
model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/fine_tuning_3_class_imbal/vgg_3block_3_class_run3/')
model.save('/content/drive/MyDrive/CEPE/dementiaproject/models_colab/fine_tuning_3_class_imbal/vgg_3block_3_class_run3/vgg_3block_3_class_run3.h5')

import pandas as pd

# convert the history.history dict to a pandas DataFrame:     
hist_df = pd.DataFrame(history.history) 

# save to json:  
hist_json_file = '/content/drive/MyDrive/CEPE/dementiaproject/models_colab/fine_tuning_3_class_imbal/vgg_3block_3_class_run3/history.json' 
with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)

# or save to csv: 
hist_csv_file = '/content/drive/MyDrive/CEPE/dementiaproject/models_colab/fine_tuning_3_class_imbal/vgg_3block_3_class_run3/history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)


# Plotting the training and validation loss
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs_0 = range(1, len(history_dict['categorical_accuracy']) + 1)
plt.plot(epochs_0, loss_values, 'bo', label='Training loss')
plt.plot(epochs_0, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss (feature-extract)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


# Plotting the training and validation accuracy
acc_values = history_dict['categorical_accuracy']
val_acc_values = history_dict['val_categorical_accuracy']
plt.plot(epochs_0, acc_values, 'bo', label='Training acc')
plt.plot(epochs_0, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy (feature-extract)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

print("prev")
import numpy

test_steps_per_epoch = numpy.math.ceil(test_generator.samples / test_generator.batch_size)

predictions = model.predict_generator(
    generator=test_generator,
    steps = test_steps_per_epoch,
    verbose = 1)

predicted_classes = numpy.argmax(predictions, axis=1)
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())   

import sklearn
report = sklearn.metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)
conf = sklearn.metrics.confusion_matrix(true_classes, predicted_classes)
print(report)   
print(conf)

#print(predicted_classes)
#print(true_classes)
#print(class_labels)

x = np.array(predicted_classes)
x_cdr1 = np.where(x  == 2)

files = np.array(test_generator.filenames)

print("predict = 0")
print(files[np.where(x  == 0)])
print("predict = 1")
print(files[np.where(x  == 1)])
print("predict = 2")
print(files[np.where(x  == 2)])

y = np.array(true_classes)
y_cdr1 = np.where(y  == 1)

#print(np.where(y  == 1))

#print(test_generator.filenames[3])

eval = model.evaluate_generator(
    test_generator,
    steps = nb_test_samples // batch_size
)

print(eval)